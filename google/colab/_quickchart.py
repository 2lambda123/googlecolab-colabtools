"""Automated chart generation for data frames."""
import itertools
import logging
import numpy as np

# Limit of underlying vega-lite schema.
# https://altair-viz.github.io/user_guide/large_datasets.html
_MAX_ROWS = 5000

_CATEGORICAL_DTYPES = (
    np.dtype('object'),
    np.dtype('bool'),
)
_DEFAULT_DATETIME_DTYPE = np.dtype('datetime64[ns]')  # a.k.a. "<M8[ns]".
_DATETIME_DTYPES = (_DEFAULT_DATETIME_DTYPE,)
_DATETIME_DTYPE_KINDS = ('M',)  # More general set of datetime dtypes.
_DATETIME_COLNAME_PATTERNS = (
    'date',
    'datetime',
    'time',
    'timestamp',
)  # Prefix/suffix matches.
_DATETIME_COLNAMES = ('dt', 't', 'ts', 'year')  # Exact matches.
_EXPECTED_DTYPES = _CATEGORICAL_DTYPES + _DATETIME_DTYPES
_CATEGORICAL_LARGE_SIZE_THRESHOLD = 8  # Facet-friendly size limit.

_DATAFRAME_REGISTRY = None


def get_registered_df(df_varname):
  """Gets a dataframe that has been previously registered.

  Args:
    df_varname: (str) A string-based key denoting the dataframe.

  Returns:
    (pd.DataFrame) A dataframe.

  Raises:
    KeyError: when the specified dataframe has not been registered.
  """
  if _DATAFRAME_REGISTRY is None:
    raise KeyError(f'Dataframe "{df_varname}" is not registered')
  return _DATAFRAME_REGISTRY[df_varname]


def find_charts(
    df,
    max_chart_instances=None,
    max_rows=_MAX_ROWS,
    random_state=0,
):
  """Finds charts compatible with dtypes of the given data frame.

  Args:
    df: (pd.DataFrame) A dataframe.
    max_chart_instances: (int) For a single chart type, the max number instances
      to generate.
    max_rows: (int) The maximum number of rows to sample from the dataframe; if
      more than `max_rows` are available, the dataframe is sampled, truncated,
      and re-sorted according to the dataframe's original index.
    random_state: (int) The random state to use when downsampling dataframes
      that exceed the `max_rows` threshold.

  Returns:
    (iterable<ChartSection>) A sequence of chart sections.
  """
  # Lazy import to avoid loading altair and transitive deps on kernel init.
  from google.colab import _quickchart_helpers  # pylint: disable=g-import-not-at-top

  global _DATAFRAME_REGISTRY
  if _DATAFRAME_REGISTRY is None:
    _DATAFRAME_REGISTRY = _quickchart_helpers.DataframeRegistry()

  if len(df) > max_rows:
    print(
        f'Warning: dataframe has {len(df)} rows, subsampling to {max_rows} '
        'due to altair plotting limitation'
    )
    df = df.sample(n=max_rows, random_state=random_state).sort_index()
  df = _coerce_datetime_columns(df)
  dtype_groups = _classify_dtypes(df)
  numeric_cols = dtype_groups['numeric']
  categorical_cols = dtype_groups['categorical']
  timelike_cols = dtype_groups['timelike']
  datetime_cols = dtype_groups['datetime']
  chart_sections = []

  if numeric_cols:
    selected_numeric_cols = numeric_cols[:max_chart_instances]
    chart_sections += [
        _quickchart_helpers.histograms_section(
            df, selected_numeric_cols, _DATAFRAME_REGISTRY
        ),
        _quickchart_helpers.value_plots_section(
            df, selected_numeric_cols, _DATAFRAME_REGISTRY
        ),
    ]

  if categorical_cols:
    selected_categorical_cols = categorical_cols[:max_chart_instances]
    chart_sections += [
        _quickchart_helpers.categorical_histograms_section(
            df, selected_categorical_cols, _DATAFRAME_REGISTRY
        ),
    ]

  if len(numeric_cols) >= 2:
    chart_sections += [
        _quickchart_helpers.linked_scatter_section(
            df,
            _select_first_k_pairs(numeric_cols, k=max_chart_instances),
            _DATAFRAME_REGISTRY,
        ),
    ]

  if len(categorical_cols) >= 2:
    chart_sections += [
        _quickchart_helpers.heatmaps_section(
            df,
            _select_first_k_pairs(categorical_cols, k=max_chart_instances),
            _DATAFRAME_REGISTRY,
        ),
    ]

  if categorical_cols and numeric_cols:
    chart_sections += [
        _quickchart_helpers.swarm_plots_section(
            df,
            _select_faceted_numeric_cols(
                numeric_cols, categorical_cols, k=max_chart_instances
            ),
            _DATAFRAME_REGISTRY,
        ),
    ]

  if timelike_cols or datetime_cols:
    chart_sections.append(
        _quickchart_helpers.time_series_line_plots_section(
            df,
            _select_time_series_cols(
                timelike_cols=timelike_cols,
                datetime_cols=datetime_cols,
                numeric_cols=numeric_cols,
                categorical_cols=categorical_cols,
                k=max_chart_instances,
            ),
            _DATAFRAME_REGISTRY,
        ),
    )

  if not chart_sections:
    print('No charts were generated by quickchart')
  return chart_sections


def _select_first_k_pairs(colnames, k=None):
  """Selects the first k pairs of column names, sequentially.

  e.g., ['a', 'b', 'c'] => [('a', b'), ('b', 'c')] for k=2

  Args:
    colnames: (iterable<str>) Column names from which to generate pairs.
    k: (int) The number of column pairs.

  Returns:
    (list<(str, str)>) A k-length sequence of column name pairs.
  """
  return itertools.islice(itertools.pairwise(colnames), k)


def _select_faceted_numeric_cols(numeric_cols, categorical_cols, k=None):
  """Selects numeric columns and corresponding categorical facets.

  Args:
    numeric_cols: (iterable<str>) Available numeric columns.
    categorical_cols: (iterable<str>) Available categorical columns.
    k: (int) The number of column pairs to select.

  Returns:
    (iter<(str, str)>) Prioritized sequence of (numeric, categorical) column
    pairs.
  """
  return itertools.islice(itertools.product(numeric_cols, categorical_cols), k)


def _select_time_series_cols(
    timelike_cols, datetime_cols, numeric_cols, categorical_cols, k=None
):
  """Selects combinations of colnames that can be plotted as time series.

  Args:
    timelike_cols: (iter<str>) Available time-like columns.
    datetime_cols: (iter<str>) Available datetime columns.
    numeric_cols: (iter<str>) Available numeric columns.
    categorical_cols: (iter<str>) Available categorical columns.
    k: (int) The number of combinations to select.

  Returns:
    (iter<(str, str, str)>) Prioritized sequence of (time, value, series)
    colname combinations.
  """
  time_cols = datetime_cols + timelike_cols
  numeric_cols = [c for c in numeric_cols if c not in time_cols]
  numeric_aggregates = ['count()']
  if not categorical_cols:
    categorical_cols = [None]
  return itertools.islice(
      itertools.product(
          time_cols, numeric_cols + numeric_aggregates, categorical_cols
      ),
      k,
  )


def _coerce_datetime_columns(df):
  """Attempts to coerce time-related columns to datetime dtype.

  Args:
    df: (pd.DataFrame) A dataframe.

  Returns:
    (pd.DataFrame) A dataframe, possibly with one or more columns having a
    modified dtype relative to the input dtypes.
  """
  # Lazy import to avoid loading pandas and transitive deps on kernel init.
  import pandas as pd  # pylint: disable=g-import-not-at-top

  def maybe_datetime(series):
    return any(
        [
            series.name.lower().startswith(p) or series.name.lower().endswith(p)
            for p in _DATETIME_COLNAME_PATTERNS
        ]
    ) or any([series.name.lower() == c for c in _DATETIME_COLNAMES])

  def as_datetime(series):
    # Support numeric-valued year.
    if series.name == 'year' and series.dtype.kind == 'i':
      return pd.to_datetime(series.astype('str'))
    # Support seconds since unix epoch.
    if 'timestamp' in series.name and series.dtype.kind == 'f':
      return pd.to_datetime(series, unit='s')
    return pd.to_datetime(series)

  df = df.copy()
  for c in df.columns:
    if maybe_datetime(df[c]):
      try:  # Just keep going if any particular column fails to convert.
        df[c] = as_datetime(df[c])
      except Exception:  # pylint: disable=broad-except
        continue
  return df


def _classify_dtypes(
    df,
    categorical_dtypes=_CATEGORICAL_DTYPES,
    datetime_dtypes=_DATETIME_DTYPES,
    datetime_dtype_kinds=_DATETIME_DTYPE_KINDS,
    categorical_size_threshold=_CATEGORICAL_LARGE_SIZE_THRESHOLD,
):
  """Classifies each dataframe series into a datatype group.

  Args:
    df: (pd.DataFrame) A dataframe.
    categorical_dtypes: (iterable<str>) Categorical data types.
    datetime_dtypes: (iterable<str>) Datetime data types.
    datetime_dtype_kinds: (iterable<str>) Datetime dtype.kind values.
    categorical_size_threshold: (int) The max number of unique values for a
      given categorical to be considered "small".

  Returns:
    ({str: list<str>}) A dict mapping a dtype name to the corresponding
    column names.
  """
  # Lazy import to avoid loading pandas and transitive deps on kernel init.
  import pandas as pd  # pylint: disable=g-import-not-at-top

  dtypes = (
      pd.DataFrame(df.dtypes, columns=['colname_dtype'])
      .reset_index()
      .rename(columns={'index': 'colname'})
  )

  filtered_cols = []
  numeric_cols = []
  cat_cols = []
  datetime_cols = []
  timelike_cols = []
  singleton_cols = []
  for colname, colname_dtype in zip(dtypes.colname, dtypes.colname_dtype):
    if not all(df[colname].apply(pd.api.types.is_hashable)):
      filtered_cols.append(colname)
    elif len(df[colname].unique()) <= 1:
      singleton_cols.append(colname)
    elif colname_dtype in categorical_dtypes:
      cat_cols.append(colname)
    elif (colname_dtype in datetime_dtypes) or (
        colname_dtype.kind in datetime_dtype_kinds
    ):
      datetime_cols.append(colname)
    elif np.issubdtype(colname_dtype, np.number):
      numeric_cols.append(colname)
    else:
      filtered_cols.append(colname)
  if filtered_cols:
    logging.warning(
        'Quickchart encountered unexpected dtypes in columns: "%r"',
        (filtered_cols,),
    )

  small_cat_cols, large_cat_cols = [], []
  for colname in cat_cols:
    if len(df[colname].unique()) <= categorical_size_threshold:
      small_cat_cols.append(colname)
    else:
      large_cat_cols.append(colname)

  for colname in numeric_cols:
    if _is_monotonically_increasing((df[colname])):
      timelike_cols.append(colname)

  return {
      'numeric': numeric_cols,
      'categorical': small_cat_cols,
      'large_categorical': large_cat_cols,
      'datetime': datetime_cols,
      'timelike': timelike_cols,
      'singleton': singleton_cols,
      'filtered': filtered_cols,
  }


def _is_monotonically_increasing(series):
  return np.all(np.array(series)[:-1] <= np.array(series)[1:])


def _get_axis_bounds(series, padding_percent=0.05, zero_rtol=1e-3):
  """Gets the min/max axis bounds for a given data series.

  Args:
    series: (pd.Series) A data series.
    padding_percent: (float) The amount of padding to add to the minimal domain
      extent as a percentage of the domain size.
    zero_rtol: (float) If either min or max bound is within this relative
      tolerance to zero, don't add padding for aesthetics.

  Returns:
    (<float> min_bound, <float> max_bound)
  """
  min_bound, max_bound = series.min(), series.max()
  padding = (max_bound - min_bound) * padding_percent
  if not np.allclose(0, min_bound, rtol=zero_rtol):
    min_bound -= padding
  if not np.allclose(0, max_bound, rtol=zero_rtol):
    max_bound += padding
  return min_bound, max_bound
